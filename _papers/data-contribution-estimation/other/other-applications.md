---
layout: default
title: Other Applications
parent: Other Approaches
grand_parent: Data Contribution Estimation
nav_order: 3
---

## Applications of Other Data Contribution Estimation Methods


### 2020

<details><summary><b>Collaborative Machine Learning with Incentive-Aware Model Rewards</b> <br>
&emsp;<i>Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, Bryan Kian Hsiang Low</i><br>
&emsp;<i>Proceedings of the 37th International Conference on Machine Learning (ICML), 2020</i><br>&emsp;
[<a target="_blank" rel="noopener noreferrer" href="https://proceedings.mlr.press/v119/sim20a">Paper</a>]
[<a target="_blank" rel="noopener noreferrer" href="https://slideslive.com/38928561/collaborative-machine-learning-with-incentiveaware-model-rewards?ref=speaker-17323">Talk</a>]
<br><br></summary>

<blockquote> <b>Abstract:</b> Collaborative machine learning (ML) is an appealing paradigm to build high-quality ML models by training on the aggregated data from many parties. However, these parties are only willing to share their data when given enough incentives, such as a guaranteed fair reward based on their contributions. This motivates the need for measuring a party’s contribution and designing an incentive-aware reward scheme accordingly. This paper proposes to value a party’s reward based on Shapley value and information gain on model parameters given its data. Subsequently, we give each party a model as a reward. To formally incentivize the collaboration, we define some desirable properties (e.g., fairness and stability) which are inspired by cooperative game theory but adapted for our model reward that is uniquely freely replicable. Then, we propose a novel model reward scheme to satisfy fairness and trade off between the desirable properties via an adjustable parameter. The value of each party’s model reward determined by our scheme is attained by injecting Gaussian noise to the aggregated training data with an optimized noise variance. We empirically demonstrate interesting properties of our scheme and evaluate its performance using synthetic and real-world datasets.
<br><br>

<!--
<details><summary><b>Notes</b></summary>TEXT
<br><br></details>
-->

<details><summary><b>Bibtex</b></summary>
{% raw %}
<pre><code> 
@InProceedings{pmlr-v119-sim20a,
  title = 	 {Collaborative Machine Learning with Incentive-Aware Model Rewards},
  author =       {Sim, Rachael Hwee Ling and Zhang, Yehong and Chan, Mun Choon and Low, Bryan Kian Hsiang},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8927--8936},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sim20a/sim20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/sim20a.html}
  } </code></pre>
{% endraw %}
</details>
</blockquote></details>