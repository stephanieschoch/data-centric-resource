---
layout: page
title: Other Methods
parent: Other
grand_parent: Data Contribution Estimation
nav_order: 2
---

## Other Methods for Data Contribution Estimation
{: .no_toc }

<!--
## Table of contents
{: .no_toc .text-delta }
-->

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
- TOC
{:toc}
</details>


### 2020
<a id="2020-other"></a>

<details><summary><b>Data Valuation using Reinforcement Learning</b> 
<br>
&emsp;<i>Jinsung Yoon, Sercan Arik, Tomas Pfister</i>
<br>
&emsp;<i>Proceedings of the 37th International Conference on Machine Learning (ICML), 2020</i>
<br>&emsp;
[<a target="_blank" rel="noopener noreferrer" href="https://proceedings.mlr.press/v119/yoon20a.html">Paper</a>]
[<a target="_blank" rel="noopener noreferrer" href="https://github.com/google-research/google-research/tree/master/dvrl">Code</a>]
<br>
<br>
</summary>
  <blockquote> <b>Abstract:</b> Quantifying the value of data is a fundamental problem in machine learning and has multiple important use cases: (1) building insights about the dataset and task, (2) domain adaptation, (3) corrupted sample discovery, and (4) robust learning. We propose Data Valuation using Reinforcement Learning (DVRL), to adaptively learn data values jointly with the predictor model. DVRL uses a data value estimator (DVE) to learn how likely each datum is used in training of the predictor model. DVE is trained using a reinforcement signal that reflects performance on the target task. We demonstrate that DVRL yields superior data value estimates compared to alternative methods across numerous datasets and application scenarios. The corrupted sample discovery performance of DVRL is close to optimal in many regimes (i.e. as if the noisy samples were known apriori), and for domain adaptation and robust learning DVRL significantly outperforms state-of-the-art by 14.6% and 10.8%, respectively.
<br><br>

<details><summary><b>Bibtex</b></summary>
{% raw %}
<pre><code> @InProceedings{pmlr-v119-yoon20a,
  title = 	 {Data Valuation using Reinforcement Learning},
  author =       {Yoon, Jinsung and Arik, Sercan and Pfister, Tomas},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10842--10851},
  year = 	 {2020},
  editor = 	 {III, Hal Daum√© and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/yoon20a/yoon20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/yoon20a.html}
}
</code></pre>
{% endraw %}
</details>
</blockquote>
</details>


<details><summary><b>TITLE</b> 
<br>
&emsp;<i>AUTHORS</i>
<br>
&emsp;<i>VENUE</i>
<br>&emsp;
[<a target="_blank" rel="noopener noreferrer" href="">Paper</a>]
[<a target="_blank" rel="noopener noreferrer" href="">Code</a>]
<br>
<br>
</summary>
  <blockquote> <b>Abstract:</b> TEXT
<br><br>

<details><summary><b>Notes</b></summary>TEXT
<br><br></details>

<details><summary><b>Bibtex</b></summary>
{% raw %}
<pre><code> BIBTEX
}</code></pre>
{% endraw %}
</details>
</blockquote>
</details>


<details><summary><b>TITLE</b> 
<br>
&emsp;<i>AUTHORS</i>
<br>
&emsp;<i>VENUE</i>
<br>&emsp;
[<a target="_blank" rel="noopener noreferrer" href="">Paper</a>]
[<a target="_blank" rel="noopener noreferrer" href="">Code</a>]
<br>
<br>
</summary>
  <blockquote> <b>Abstract:</b> TEXT
<br><br>

<details><summary><b>Notes</b></summary>TEXT
<br><br></details>

<details><summary><b>Bibtex</b></summary>
{% raw %}
<pre><code> BIBTEX
}</code></pre>
{% endraw %}
</details>
</blockquote>
</details>
